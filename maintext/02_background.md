# Background

> The problem, and it is a moral problem, with which we are here faced is very
> close to one of the great problems of slavery. Let us grant that slavery is bad
> because it is cruel. It is, how-ever, self-contradictory, and for a reason
> which is quite different. We wish a slave to be intelligent, to be able to
> assist us in the carrying out of our tasks. However, we also wish him to be
> subservient. Complete subservience and complete intelligence do not go
> together.
>
> \- Norbert Wiener [@wienerMoralTechnicalConsequences1960]

Even though the history of Large Language Models (LLMs) is relatively young, the
shoulders on which it stands are not. Questions regarding the specificity of
goals and machines are first researched in the previous century
[@wienerMoralTechnicalConsequences1960]. Forward thinkers such as Norbert Wiener
managed to ask questions that are not only relevant today, but also hugely
important. As discussed before, the effects of jailbreaks can be harmful, but
their underlying cause should raise even more concern. It is paramount that we
determine how AI fails us, and how we can control it; how we align AI with our
goals and values.

In this section, we will discuss how we currently understand AI alignment,
starting from the ground up. We will begin with the inner workings of LLMs (see
[Large Language Models]), how they are trained, and fine-tuned.  Next, we will
discuss jailbreaks (see [Jailbreaking]), specifically previously discovered
jailbreaks, and the threats they pose. Then, we will cover how safety training
is actually performed (see [Safety Training]), which means discussing
reinforcement learning, and reinforcement learning through human feedback
(RLHF). Finally, we discuss the concept of AI alignment (see [AI Alignment]),
the idea of outer versus inner alignment, and recent work on aligning LLMs in
the context of safety training.

## Large Language Models

### The Transformer

### Training

### Fine-tuning

## Jailbreaking

### Classifications

### Threats

## Safety Training

### Reinforcement Learning

### Reinforcement Learning through Human Feedback

## AI Alignment

### Inner versus Outer Alignment
