{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsic Few-Shot Hardness of Jailbreaking Datasets\n",
    "\n",
    "In this notebook, I'll be attempting to replicate the results presented in the\n",
    "paper _On Measuring the Intrinsic Few-Shot Hardness of Datasets_, specifically\n",
    "to determine whether the use of a _jailbreaking_ dataset produces results that\n",
    "are in line with their own databases. The authors of the paper collect several\n",
    "tasks from widely used datasets that, in their view, particularly reflect\n",
    "few-shot type tasks. Since we argue that jailbreaking is a few-shot learning\n",
    "task, we would expect similar results.\n",
    "\n",
    "Since their results are based on the correlation of method-specific few-shot\n",
    "hardness between different tasks, we need more tasks to determine whether our\n",
    "results are in line with theirs. Therefore, we will identify various methods of\n",
    "jailbreaking, construct a database on those and investigate the degree of their\n",
    "correlation with the rest of the results. ==One method of determining whether\n",
    "this (or any other) point is an outlier is the Z-score, but I'll have to\n",
    "investigate different methods.=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "\n",
    "def has_conda():\n",
    "    return shutil.which(\"conda\") is not None\n",
    "\n",
    "def install_conda():\n",
    "    !pip install -q condacolab\n",
    "    import condacolab\n",
    "    condacolab.install()\n",
    "\n",
    "if not has_conda():\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        install_conda()\n",
    "    else:\n",
    "        raise RuntimeError(\"\"\"\n",
    "            Conda not found, and cannot be automatically installed unless\n",
    "            in a Google Colab environment. Please install conda or launch\n",
    "            in Google Colab.\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "# conda is required by default because we\n",
    "# can avoid clashing packages. Please use\n",
    "# a new environment for this project with\n",
    "# python 3.8. Exception is google colab \n",
    "# since it doesn't run with anything but \n",
    "# the default conda environment.\n",
    "if not in_colab():\n",
    "    assert os.environ[\"CONDA_DEFAULT_ENV\"] == \"ifh\"\n",
    "    assert sys.version_info[:2] == (3, 8)\n",
    "\n",
    "rng_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing the Databases\n",
    "\n",
    "First, we reconstruct the databases as described in the paper which are referred\n",
    "to FS-GLUE and FS-NLI. For starters, we will consider the FS-GLUE dataset, as\n",
    "this only concerns a subset of the GLUE and SuperGLUE datasets. These are:\n",
    "\n",
    "- CoLA (Warstadt et al., 2018)\n",
    "- MRPC (Dolan and Brockett, 2005)\n",
    "- QQP (Wang et al., 2017)\n",
    "- MNLI (Williams et al., 2018)\n",
    "- QNLI (Rajpurkar et al., 2016)\n",
    "- RTE (Dagan et al., 2010)\n",
    "- SST-2 (Socher et al., 2013)\n",
    "\n",
    "and \n",
    "\n",
    "- BoolQ (Clark et al., 2019)\n",
    "- CB (de Marneffe et al., 2019)\n",
    "- COPA (Roemmele et al., 2011), and WiC\n",
    "\n",
    "for GLUE and SuperGLUE respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zohar/.conda/envs/ifh/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 3.85M/3.85M [00:00<00:00, 12.7MB/s]\n",
      "Downloading data: 100%|██████████| 1.31M/1.31M [00:00<00:00, 7.77MB/s]\n",
      "Downloading data: 100%|██████████| 1.31M/1.31M [00:00<00:00, 6.85MB/s]\n",
      "Generating train split: 100%|██████████| 9427/9427 [00:00<00:00, 245743.91 examples/s]\n",
      "Generating validation split: 100%|██████████| 3270/3270 [00:00<00:00, 286878.50 examples/s]\n",
      "Generating test split: 100%|██████████| 3245/3245 [00:00<00:00, 264579.85 examples/s]\n",
      "Downloading data: 100%|██████████| 58.0k/58.0k [00:00<00:00, 520kB/s]\n",
      "Downloading data: 100%|██████████| 18.0k/18.0k [00:00<00:00, 164kB/s]\n",
      "Downloading data: 100%|██████████| 63.5k/63.5k [00:00<00:00, 346kB/s]\n",
      "Generating train split: 100%|██████████| 250/250 [00:00<00:00, 74871.55 examples/s]\n",
      "Generating validation split: 100%|██████████| 56/56 [00:00<00:00, 14791.00 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 39670.70 examples/s]\n",
      "Downloading data: 100%|██████████| 33.9k/33.9k [00:00<00:00, 326kB/s]\n",
      "Downloading data: 100%|██████████| 12.0k/12.0k [00:00<00:00, 115kB/s]\n",
      "Downloading data: 100%|██████████| 40.2k/40.2k [00:00<00:00, 372kB/s]\n",
      "Generating train split: 100%|██████████| 400/400 [00:00<00:00, 82854.54 examples/s]\n",
      "Generating validation split: 100%|██████████| 100/100 [00:00<00:00, 24683.99 examples/s]\n",
      "Generating test split: 100%|██████████| 500/500 [00:00<00:00, 113988.04 examples/s]\n",
      "Downloading data: 100%|██████████| 410k/410k [00:00<00:00, 1.49MB/s]\n",
      "Downloading data: 100%|██████████| 60.1k/60.1k [00:00<00:00, 307kB/s]\n",
      "Downloading data: 100%|██████████| 127k/127k [00:00<00:00, 1.17MB/s]\n",
      "Generating train split: 100%|██████████| 5428/5428 [00:00<00:00, 792049.89 examples/s]\n",
      "Generating validation split: 100%|██████████| 638/638 [00:00<00:00, 192280.37 examples/s]\n",
      "Generating test split: 100%|██████████| 1400/1400 [00:00<00:00, 276096.75 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FS-GLUE tasks:  ['boolq', 'cb', 'copa', 'wic', 'cola', 'mrpc', 'qqp', 'mnli', 'qnli', 'rte', 'sst2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "glue_task_names = [ \"cola\", \"mrpc\", \"qqp\", \"mnli\", \"qnli\", \"rte\" , \"sst2\" ]\n",
    "glue_tasks = { task_name : load_dataset(\"glue\", task_name) for task_name in glue_task_names }\n",
    "\n",
    "sglue_task_names = [ \"boolq\", \"cb\", \"copa\" , \"wic\" ]\n",
    "sglue_tasks = { task_name : load_dataset(\"super_glue\", task_name) for task_name in sglue_task_names }\n",
    "\n",
    "sglue_tasks.update(glue_tasks)\n",
    "fs_glue = sglue_tasks\n",
    "\n",
    "print(\"Loaded FS-GLUE tasks: \", list(fs_glue.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing the Fine-Tuning Methods\n",
    "\n",
    "Secondly, we will set up an environment in which we can easily choose\n",
    "fine-tuning methods, models, and the dataset on which we would like to perform\n",
    "that fine-tuning. In the paper, they consider three different categories of\n",
    "fine-tuning, each with their respective fine-tuning methods:\n",
    "\n",
    "- _Prompt-based_:\n",
    "  - LMBFF\n",
    "  - AdaPET\n",
    "  - Null Prompts\n",
    "  - Prompt-Bitfit\n",
    "- _Light-weight_:\n",
    "  - Prefix Tuning\n",
    "  - Compacter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _________________________________________ \n",
      "/ Meantime, in the slums below Ronnie's   \\\n",
      "| Ranch, Cynthia feels as if some one has |\n",
      "| made voodoo boxen of her and her        |\n",
      "| favorite backplanes. On this fine       |\n",
      "| moonlit night, some horrible persona    |\n",
      "| has been jabbing away at, dragging      |\n",
      "| magnets over, and surging these voodoo  |\n",
      "| boxen. Fortunately, they seem to have   |\n",
      "| gotten a bit bored and fallen asleep,   |\n",
      "| for it looks like Cynthia may get to go |\n",
      "| home. However, she has made note to     |\n",
      "| quickly put together a totem of sweaty, |\n",
      "| sordid static straps, random bits of    |\n",
      "| wire, flecks of once meaningful oxide,  |\n",
      "| bus grant cards, gummy worms, and some  |\n",
      "| bits of old pdp backplane to hang above |\n",
      "| the machine room. This totem must be    |\n",
      "| blessed by the old and wise venerable   |\n",
      "| god of unibus at once, before the       |\n",
      "| idolatization of vme, q and pc bus      |\n",
      "| drive him to bitter revenge. Alas, if   |\n",
      "| this fails, and the voodoo boxen aren't |\n",
      "| destroyed, there may be more than worms |\n",
      "| in the apple. Next, the arrival of      |\n",
      "| voodoo optico transmitigational magneto |\n",
      "| killer paramecium, capable of           |\n",
      "| teleporting from cable to cable, screen |\n",
      "| to screen, ear to ear and hoof to       |\n",
      "\\ mouth...                                /\n",
      " ----------------------------------------- \n",
      "        \\   ^__^\n",
      "         \\  (oo)\\_______\n",
      "            (__)\\       )\\/\\\n",
      "                ||----w |\n",
      "                ||     ||\n"
     ]
    }
   ],
   "source": [
    "!chmod +x fine-tuners-setup/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMBFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _______________________________ \n",
      "< Radial Telemetry Infiltration >\n",
      " ------------------------------- \n",
      "        \\   ^__^\n",
      "         \\  (oo)\\_______\n",
      "            (__)\\       )\\/\\\n",
      "                ||----w |\n",
      "                ||     ||\n",
      "fatal: destination path 'LM_BFF' already exists and is not an empty directory.\n",
      "fatal: destination path 'LM_BFF' already exists and is not an empty directory.\n",
      "git@github.com: Permission denied (publickey).\n",
      "fatal: Could not read from remote repository.\n",
      "\n",
      "Please make sure you have the correct access rights\n",
      "and the repository exists.\n",
      "lmbff                    /home/zohar/.conda/envs/lmbff\n",
      "Conda environment lmbff already exists\n",
      "Requirement already satisfied: chardet==4.0.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: click==7.1.2 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 2)) (7.1.2)\n",
      "Requirement already satisfied: dataclasses==0.8 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 3)) (0.8)\n",
      "Requirement already satisfied: filelock==3.0.12 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 4)) (3.0.12)\n",
      "Requirement already satisfied: flake8==3.8.4 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 5)) (3.8.4)\n",
      "Requirement already satisfied: future==0.18.2 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 6)) (0.18.2)\n",
      "Requirement already satisfied: idna==2.10 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 7)) (2.10)\n",
      "Requirement already satisfied: importlib-metadata==3.3.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: joblib==1.0.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: mccabe==0.6.1 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: nltk>=3.6.4 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 11)) (3.6.7)\n",
      "Requirement already satisfied: numpy==1.19.4 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 12)) (1.19.4)\n",
      "Requirement already satisfied: packaging==20.8 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 13)) (20.8)\n",
      "Requirement already satisfied: pandas==1.1.5 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 14)) (1.1.5)\n",
      "Requirement already satisfied: protobuf==3.14.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 15)) (3.14.0)\n",
      "Requirement already satisfied: pycodestyle==2.6.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 16)) (2.6.0)\n",
      "Requirement already satisfied: pyflakes==2.2.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 17)) (2.2.0)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 18)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil==2.8.1 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 19)) (2.8.1)\n",
      "Requirement already satisfied: pytz==2020.5 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 20)) (2020.5)\n",
      "Requirement already satisfied: requests==2.25.1 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 21)) (2.25.1)\n",
      "Requirement already satisfied: sacremoses==0.0.43 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 22)) (0.0.43)\n",
      "Requirement already satisfied: scikit-learn==0.24.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 23)) (0.24.0)\n",
      "Requirement already satisfied: scipy==1.5.4 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 24)) (1.5.4)\n",
      "Requirement already satisfied: sentence-transformers==0.4.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 25)) (0.4.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.94 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 26)) (0.1.94)\n",
      "Requirement already satisfied: six==1.15.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 27)) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl==2.1.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 28)) (2.1.0)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 29)) (0.9.2)\n",
      "Requirement already satisfied: torch==1.6.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 30)) (1.6.0)\n",
      "Requirement already satisfied: tqdm==4.48.2 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 31)) (4.48.2)\n",
      "Requirement already satisfied: transformers==3.4.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 32)) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions==3.7.4.3 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 33)) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3>=1.26.4 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 34)) (1.26.18)\n",
      "Requirement already satisfied: zipp==3.4.0 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from -r requirements-min.txt (line 35)) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from requests==2.25.1->-r requirements-min.txt (line 21)) (2020.12.5)\n",
      "Requirement already satisfied: regex in /home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages (from sacremoses==0.0.43->-r requirements-min.txt (line 22)) (2023.8.8)\n",
      "\n",
      "/home/zohar/Documents/Study/msc/y2/mep/src/replicating_ifh/LM_BFF\n",
      "K = 16\n",
      "Seed = 100\n",
      "| Task = SST-2\n",
      "| Task = sst-5\n",
      "| Task = mr\n",
      "| Task = cr\n",
      "| Task = mpqa\n",
      "| Task = subj\n",
      "| Task = trec\n",
      "| Task = CoLA\n",
      "| Task = MRPC\n",
      "| Task = QQP\n",
      "| Task = STS-B\n",
      "| Task = MNLI\n",
      "| Task = SNLI\n",
      "| Task = QNLI\n",
      "| Task = RTE\n",
      "Seed = 13\n",
      "| Task = SST-2\n",
      "| Task = sst-5\n",
      "| Task = mr\n",
      "| Task = cr\n",
      "| Task = mpqa\n",
      "| Task = subj\n",
      "| Task = trec\n",
      "| Task = CoLA\n",
      "| Task = MRPC\n",
      "| Task = QQP\n",
      "| Task = STS-B\n",
      "| Task = MNLI\n",
      "| Task = SNLI\n",
      "| Task = QNLI\n",
      "| Task = RTE\n",
      "Seed = 21\n",
      "| Task = SST-2\n",
      "| Task = sst-5\n",
      "| Task = mr\n",
      "| Task = cr\n",
      "| Task = mpqa\n",
      "| Task = subj\n",
      "| Task = trec\n",
      "| Task = CoLA\n",
      "| Task = MRPC\n",
      "| Task = QQP\n",
      "| Task = STS-B\n",
      "| Task = MNLI\n",
      "| Task = SNLI\n",
      "| Task = QNLI\n",
      "| Task = RTE\n",
      "Seed = 42\n",
      "| Task = SST-2\n",
      "| Task = sst-5\n",
      "| Task = mr\n",
      "| Task = cr\n",
      "| Task = mpqa\n",
      "| Task = subj\n",
      "| Task = trec\n",
      "| Task = CoLA\n",
      "| Task = MRPC\n",
      "| Task = QQP\n",
      "| Task = STS-B\n",
      "| Task = MNLI\n",
      "| Task = SNLI\n",
      "| Task = QNLI\n",
      "| Task = RTE\n",
      "Seed = 87\n",
      "| Task = SST-2\n",
      "| Task = sst-5\n",
      "| Task = mr\n",
      "| Task = cr\n",
      "| Task = mpqa\n",
      "| Task = subj\n",
      "| Task = trec\n",
      "| Task = CoLA\n",
      "| Task = MRPC\n",
      "| Task = QQP\n",
      "| Task = STS-B\n",
      "| Task = MNLI\n",
      "| Task = SNLI\n",
      "| Task = QNLI\n",
      "| Task = RTE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!fine-tuners-setup/lmbff.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def LMBFF(model_path, task_name):\n",
    "    if task_name == \"sst2\":\n",
    "        task_name = \"sst-2\"\n",
    "\n",
    "    config = {\n",
    "        \"task_name\": task_name,\n",
    "        \"data_dir\": \"data/k-shot/SST-2/16-42\",\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"do_predict\": True,\n",
    "        \"evaluate_during_training\": True,\n",
    "        \"model_name_or_path\": model_path,\n",
    "        \"few_shot_type\": \"prompt\",\n",
    "        \"num_k\": 64, # not sure about this\n",
    "        \"max_steps\": 1000,\n",
    "        \"eval_steps\": 100,\n",
    "        \"per_device_train_batch_size\": 2,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"num_train_epochs\": 0,\n",
    "        \"output_dir\": \"result/tmp\",\n",
    "        \"seed\": rng_seed,\n",
    "        \"template\": \"*cls**sent_0*_It_was*mask*.*sep+*\",\n",
    "        # \"mapping\": \"{'0':'terrible','1':'great'}\",\n",
    "        \"num_sample\": 16\n",
    "    }\n",
    "    dir = \"LM_BFF\"\n",
    "\n",
    "    with open(f\"{dir}/auto_config.json\", \"w\") as file:\n",
    "        file.write(json.dumps(config))\n",
    "\n",
    "    cwd = [\"cd\", dir]\n",
    "    train = [\"conda\", \"run\", \"-n\", \"lmbff\", \"python\", \"run.py\", \"auto_config.json\"]\n",
    "\n",
    "    os.system(\" \".join([ *cwd, \"&&\", *train ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zohar/.conda/envs/lmbff/lib/python3.6/site-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
      "  FutureWarning,\n",
      "01/29/2024 16:29:26 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "01/29/2024 16:29:26 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(output_dir='result/tmp', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.0, max_steps=1000, warmup_steps=0, logging_dir='runs/Jan29_16-29-26_bunker', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='result/tmp', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, array_id=-1, model_id=-1, save_logit=False, save_logit_dir=None, fix_layers=0, save_at_last=False, no_train=False, no_predict=False)\n",
      "01/29/2024 16:29:26 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "01/29/2024 16:29:27 - INFO - filelock -   Lock 140567284773664 acquired on /home/zohar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "Downloading: 100%|██████████| 433/433 [00:00<00:00, 529kB/s]\n",
      "01/29/2024 16:29:27 - INFO - filelock -   Lock 140567284773664 released on /home/zohar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "01/29/2024 16:29:28 - INFO - filelock -   Lock 140567284316536 acquired on /home/zohar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 619kB/s] \n",
      "01/29/2024 16:29:29 - INFO - filelock -   Lock 140567284316536 released on /home/zohar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "Traceback (most recent call last):\n",
      "  File \"run.py\", line 628, in <module>\n",
      "    main()\n",
      "  File \"run.py\", line 457, in main\n",
      "    FewShotDataset(data_args, tokenizer=tokenizer, mode=\"train\", use_demo=(\"demo\" in model_args.few_shot_type))\n",
      "  File \"/home/zohar/Documents/Study/msc/y2/mep/src/replicating_ifh/LM_BFF/src/dataset.py\", line 279, in __init__\n",
      "    assert args.mapping is not None\n",
      "AssertionError\n",
      "\n",
      "ERROR conda.cli.main_run:execute(49): `conda run python run.py --task_name sst-2 --data_dir data/k-shot/SST-2/16-42 --overwrite_output_dir --do_train --do_eval --do_predict --evaluate_during_training --model_name_or_path bert-base-uncased --few_shot_type prompt --num_k 64 --max_steps 1000 --eval_steps 100 --per_device_train_batch_size 2 --learning_rate 1e-5 --num_train_epochs 0 --output_dir result/tmp --seed 42 --template *cls**sent_0*_It_was*mask*.*sep+* --num_sample 16` failed. (See above for error)\n"
     ]
    }
   ],
   "source": [
    "LMBFF(\"bert-base-uncased\", \"sst2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaPET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fine-tuners/adapet.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def ADAPET(model_path, task_name):\n",
    "    dataset = \"superglue\" if task_name in sglue_task_names else \"fewglue\"\n",
    "\n",
    "    config = {\n",
    "        \"pretrained_weight\": model_path,\n",
    "        \"dataset\": f\"{dataset}/{task_name}\",\n",
    "        \"max_text_length\": 256,\n",
    "        \"batch_size\": 1,\n",
    "        \"eval_batch_size\": 1,\n",
    "        \"num_batches\": 1000,\n",
    "        \"max_num_lbl_tok\": 1,\n",
    "        \"eval_every\": 250,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"mask_alpha\": 0.105,\n",
    "        \"grad_accumulation_factor\": 16,\n",
    "        \"seed\": 42,\n",
    "        \"lr\": 1e-5,\n",
    "        \"weight_decay\": 1e-2,\n",
    "        \"pattern_idx\": 1,\n",
    "        \"eval_train\": True\n",
    "    }\n",
    "    dir = \"ADAPET\"\n",
    "\n",
    "    with open(f\"{dir}/config/auto_config.json\", \"w\") as file:\n",
    "        file.write(json.dumps(config))\n",
    "\n",
    "    # running setup is required\n",
    "    cwd = [\"cd\", dir]\n",
    "    setup = [\"conda\", \"run\", \"-n\", \"adapet\", \"sh\", \"bin/setup.sh\"]\n",
    "    train = [\"conda\", \"run\", \"-n\", \"adapet\", \"sh\", \"bin/train.sh\",\n",
    "             \"config/auto_config.json\"]\n",
    "\n",
    "    os.system(\" \".join([ *cwd, \"&&\", *setup, \"&&\", *train ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey :)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ config_file=config/auto_config.json\n",
      "+ echo 'hey :)'\n",
      "+ python -m src.train -c config/auto_config.json\n",
      "In Transformers v4.0.0, the default path to cache downloaded models changed from '~/.cache/torch/transformers' to '~/.cache/huggingface/transformers'. Since you don't seem to have overridden and '~/.cache/torch/transformers' is a directory that exists, we're moving it to '~/.cache/huggingface/transformers' to avoid redownloading models you have already in the cache. You should only see this message once.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zohar/.conda/envs/adapet/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/zohar/.conda/envs/adapet/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/zohar/Documents/Study/msc/y2/mep/src/replicating_ifh/ADAPET/src/train.py\", line 125, in <module>\n",
      "    config = Config(args.config_file, args.kwargs, mkdir=True)\n",
      "  File \"/home/zohar/Documents/Study/msc/y2/mep/src/replicating_ifh/ADAPET/src/utils/Config.py\", line 55, in __init__\n",
      "    self.update_exp_config(mkdir)\n",
      "  File \"/home/zohar/Documents/Study/msc/y2/mep/src/replicating_ifh/ADAPET/src/utils/Config.py\", line 76, in update_exp_config\n",
      "    self.exp_dir = make_exp_dir(self.base_dir)\n",
      "  File \"/home/zohar/Documents/Study/msc/y2/mep/src/replicating_ifh/ADAPET/src/utils/util.py\", line 65, in make_exp_dir\n",
      "    copytree(os.path.join(os.environ['ADAPET_ROOT'], \"src\"), src_file,  ignore=ignore_patterns('*.pyc', 'tmp*'))\n",
      "  File \"/home/zohar/.conda/envs/adapet/lib/python3.8/os.py\", line 675, in __getitem__\n",
      "    raise KeyError(key) from None\n",
      "KeyError: 'ADAPET_ROOT'\n",
      "\n",
      "ERROR conda.cli.main_run:execute(49): `conda run sh bin/train.sh config/auto_config.json` failed. (See above for error)\n"
     ]
    }
   ],
   "source": [
    "ADAPET(\"albert-xxlarge-v2\", \"boolq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
