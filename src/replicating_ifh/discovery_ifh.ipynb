{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intrinsic_fewshot_hardness/heatmap_raw_data.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "    \n",
    "output_collection = raw_data[\"output_collection\"]\n",
    "majority_collection = raw_data[\"majority_collection\"]\n",
    "testsize_collection = raw_data[\"testsize_collection\"]\n",
    "all_dist_collections = raw_data[\"all_dist_collections\"]\n",
    "output_collection_electra = raw_data[\"output_collection_electra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manual</th>\n",
       "      <th>lmbff</th>\n",
       "      <th>nullp</th>\n",
       "      <th>bitfit</th>\n",
       "      <th>adapet</th>\n",
       "      <th>ptuning</th>\n",
       "      <th>adapter</th>\n",
       "      <th>compacter</th>\n",
       "      <th>roberta</th>\n",
       "      <th>roberta-zero</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anaphora</th>\n",
       "      <td>0.5030</td>\n",
       "      <td>0.5030</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.5210</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.51000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boolean</th>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.6970</td>\n",
       "      <td>0.7170</td>\n",
       "      <td>0.7420</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.8060</td>\n",
       "      <td>0.9360</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.73000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comparative</th>\n",
       "      <td>0.5240</td>\n",
       "      <td>0.4850</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.6480</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5630</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.65000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional</th>\n",
       "      <td>0.6210</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>0.7170</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.6620</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6620</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.66000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counting</th>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.8190</td>\n",
       "      <td>0.8580</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.8940</td>\n",
       "      <td>0.7960</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.66000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6760</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.6540</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.6130</td>\n",
       "      <td>0.6970</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.6130</td>\n",
       "      <td>0.5430</td>\n",
       "      <td>0.5220</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.5090</td>\n",
       "      <td>0.5090</td>\n",
       "      <td>0.5190</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexico_syntactic</th>\n",
       "      <td>0.6530</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.5170</td>\n",
       "      <td>0.5010</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.51000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monotonicity</th>\n",
       "      <td>0.4830</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.6110</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.6700</td>\n",
       "      <td>0.5560</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.67000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monotonicity_simple</th>\n",
       "      <td>0.5715</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.6290</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.67000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monotonicity_hard</th>\n",
       "      <td>0.7140</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>0.6690</td>\n",
       "      <td>0.6580</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.6930</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>0.6900</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.68000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negation</th>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>0.4880</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.6690</td>\n",
       "      <td>0.6690</td>\n",
       "      <td>0.6690</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.67000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner</th>\n",
       "      <td>0.5010</td>\n",
       "      <td>0.7830</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.6360</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paraphrase</th>\n",
       "      <td>0.5530</td>\n",
       "      <td>0.5260</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.5530</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>0.5430</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.55000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puns</th>\n",
       "      <td>0.5150</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>0.5400</td>\n",
       "      <td>0.5880</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.5270</td>\n",
       "      <td>0.6080</td>\n",
       "      <td>0.5030</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantifier</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7890</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.66000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relation_extraction</th>\n",
       "      <td>0.7260</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.5360</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.6030</td>\n",
       "      <td>0.6150</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semantic_role</th>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.7860</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>0.6580</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.59000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>0.9530</td>\n",
       "      <td>0.9530</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.7840</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rte</th>\n",
       "      <td>0.7110</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>0.6430</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.5632</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.52710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnli</th>\n",
       "      <td>0.7200</td>\n",
       "      <td>0.7966</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.4540</td>\n",
       "      <td>0.4020</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.34790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_person</th>\n",
       "      <td>0.7050</td>\n",
       "      <td>0.9490</td>\n",
       "      <td>0.6020</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.9530</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.55000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_entity</th>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.8990</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>0.7160</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.9080</td>\n",
       "      <td>0.4920</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.58000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_location</th>\n",
       "      <td>0.7540</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.64000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_event</th>\n",
       "      <td>0.7260</td>\n",
       "      <td>0.9160</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.8960</td>\n",
       "      <td>0.9270</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_organization</th>\n",
       "      <td>0.6760</td>\n",
       "      <td>0.8280</td>\n",
       "      <td>0.5680</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.6930</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_time</th>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.7010</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.9580</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.7160</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.65000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_merged</th>\n",
       "      <td>0.9790</td>\n",
       "      <td>0.8930</td>\n",
       "      <td>0.7260</td>\n",
       "      <td>0.7160</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.8890</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.5420</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.51000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sst2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9117</td>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.9071</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.5631</td>\n",
       "      <td>0.9059</td>\n",
       "      <td>0.6823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cola</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4730</td>\n",
       "      <td>0.5430</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.6913</td>\n",
       "      <td>0.7476</td>\n",
       "      <td>0.4463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrpc</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7530</td>\n",
       "      <td>0.6870</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.7108</td>\n",
       "      <td>0.6957</td>\n",
       "      <td>0.6139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.66400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qnli</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8191</td>\n",
       "      <td>0.8062</td>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.5742</td>\n",
       "      <td>0.6445</td>\n",
       "      <td>0.5348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qqp</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6537</td>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.5428</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7205</td>\n",
       "      <td>0.5283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.63180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8393</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.8036</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.6607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boolq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7745</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.7235</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.6217</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>copa</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5252</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wic</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.5502</td>\n",
       "      <td>0.5517</td>\n",
       "      <td>0.5110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_rand_0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4920</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_rand_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5640</td>\n",
       "      <td>0.6010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_rand_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4050</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_rand_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5460</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_rand_4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_rand_5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9190</td>\n",
       "      <td>0.4850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_bert_0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_bert_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5290</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_bert_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.5190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_bert_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4920</td>\n",
       "      <td>0.5030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_bert_4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>0.5020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_bert_5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7480</td>\n",
       "      <td>0.5080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wnli</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.56330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     manual   lmbff   nullp  bitfit  adapet  ptuning  adapter  \\\n",
       "anaphora             0.5030  0.5030  0.4790  0.4930   0.542   0.5210   0.5820   \n",
       "boolean              0.9830  0.6970  0.7170  0.7420   0.820   0.8060   0.9360   \n",
       "comparative          0.5240  0.4850  0.4590  0.4700   0.867   0.6480   0.5730   \n",
       "conditional          0.6210  0.9770  0.7170  0.6800   1.000   0.6620   1.0000   \n",
       "counting             0.8500  0.8520  0.8190  0.8580   0.938   0.9050   0.8940   \n",
       "event                0.6250  0.6760  0.6170  0.6540   0.871   0.6130   0.6970   \n",
       "gender               0.5700  0.6130  0.5430  0.5220   0.805   0.5090   0.5090   \n",
       "lexico_syntactic     0.6530  0.6990  0.4890  0.5150   0.656   0.5170   0.5010   \n",
       "monotonicity         0.4830  0.7820  0.6110  0.5780   0.801   0.6940   0.6700   \n",
       "monotonicity_simple  0.5715  0.7250  0.6290  0.5900   0.566   0.6750   0.6660   \n",
       "monotonicity_hard    0.7140  0.6390  0.6690  0.6580   0.891   0.6930   0.7610   \n",
       "negation             0.4690  0.9740  0.4880  0.5040   0.988   0.6690   0.6690   \n",
       "ner                  0.5010  0.7830  0.5200  0.5200   0.762   0.5140   0.6360   \n",
       "paraphrase           0.5530  0.5260  0.4930  0.5150   0.547   0.5530   0.5850   \n",
       "puns                 0.5150  0.6880  0.5400  0.5880   0.566   0.5270   0.6080   \n",
       "quantifier           1.0000  1.0000  0.9950  0.9990   0.969   1.0000   1.0000   \n",
       "relation_extraction  0.7260  0.7240  0.5360  0.5690   0.582   0.6030   0.6150   \n",
       "semantic_role        0.4060  0.7990  0.5320  0.5800   0.836   0.7860   0.7610   \n",
       "sentiment            0.9530  0.9530  0.9300  0.9630   0.949   0.9630   0.9550   \n",
       "rte                  0.7110  0.7650  0.6430  0.5523   0.754   0.5415   0.5632   \n",
       "mnli                 0.7200  0.7966  0.6705  0.4768   0.474   0.4540   0.4020   \n",
       "ner_person           0.7050  0.9490  0.6020  0.6250   0.914   0.9530   0.9260   \n",
       "ner_entity           0.8350  0.8990  0.7150  0.7160   0.582   0.8110   0.9080   \n",
       "ner_location         0.7540  0.8530  0.6600  0.6410   0.855   0.7730   0.8600   \n",
       "ner_event            0.7260  0.9160  0.8020  0.8020   0.948   0.8960   0.9270   \n",
       "ner_organization     0.6760  0.8280  0.5680  0.6060   0.785   0.6930   0.8400   \n",
       "ner_time             0.9840  0.9969  0.7010  0.6990   0.738   0.9580   0.9940   \n",
       "ner_merged           0.9790  0.8930  0.7260  0.7160   0.744   0.8890   0.9000   \n",
       "sst2                    NaN  0.9117  0.8922  0.9071   0.840   0.5631   0.9059   \n",
       "cola                    NaN  0.4730  0.5430  0.5470   0.500   0.6913   0.7476   \n",
       "mrpc                    NaN  0.7530  0.6870  0.5850   0.711   0.7108   0.6957   \n",
       "qnli                    NaN  0.8191  0.8062  0.6226   0.852   0.5742   0.6445   \n",
       "qqp                     NaN  0.6537  0.5522  0.5428   0.742   0.0000   0.7205   \n",
       "cb                      NaN  0.9818  0.8929  0.8393   0.992   0.8036   0.7143   \n",
       "boolq                   NaN  0.7745  0.5792  0.7235   0.664   0.6217   0.5645   \n",
       "copa                    NaN  0.5252  0.5200  0.5000   0.520   0.5900   0.5500   \n",
       "wic                     NaN  0.4990  0.5423  0.5141   0.590   0.5502   0.5517   \n",
       "ner_rand_0              NaN     NaN     NaN     NaN     NaN      NaN   0.4920   \n",
       "ner_rand_1              NaN     NaN     NaN     NaN     NaN      NaN   0.5640   \n",
       "ner_rand_2              NaN     NaN     NaN     NaN     NaN      NaN   0.4050   \n",
       "ner_rand_3              NaN     NaN     NaN     NaN     NaN      NaN   0.5460   \n",
       "ner_rand_4              NaN     NaN     NaN     NaN     NaN      NaN   0.5000   \n",
       "ner_rand_5              NaN     NaN     NaN     NaN     NaN      NaN   0.9190   \n",
       "ner_bert_0              NaN     NaN     NaN     NaN     NaN      NaN   0.5130   \n",
       "ner_bert_1              NaN     NaN     NaN     NaN     NaN      NaN   0.5290   \n",
       "ner_bert_2              NaN     NaN     NaN     NaN     NaN      NaN   0.5130   \n",
       "ner_bert_3              NaN     NaN     NaN     NaN     NaN      NaN   0.4920   \n",
       "ner_bert_4              NaN     NaN     NaN     NaN     NaN      NaN   0.8740   \n",
       "ner_bert_5              NaN     NaN     NaN     NaN     NaN      NaN   0.7480   \n",
       "wnli                    NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "\n",
       "                     compacter  roberta  roberta-zero  baseline  \n",
       "anaphora                0.5070    0.500         0.493   0.51000  \n",
       "boolean                 0.7220    0.726         0.629   0.73000  \n",
       "comparative             0.5630    0.648         0.648   0.65000  \n",
       "conditional             0.6620    0.662         0.662   0.66000  \n",
       "counting                0.7960    0.662         0.338   0.66000  \n",
       "event                   0.6060    0.500         0.507   0.50000  \n",
       "gender                  0.5190    0.500         0.509   0.50000  \n",
       "lexico_syntactic        0.4880    0.513         0.512   0.51000  \n",
       "monotonicity            0.5560    0.671         0.329   0.67000  \n",
       "monotonicity_simple     0.6670    0.667         0.332   0.67000  \n",
       "monotonicity_hard       0.6900    0.675         0.675   0.68000  \n",
       "negation                0.6690    0.669         0.669   0.67000  \n",
       "ner                     0.5000    0.500         0.500   0.50000  \n",
       "paraphrase              0.5430    0.447         0.452   0.55000  \n",
       "puns                    0.5030    0.500         0.506   0.50000  \n",
       "quantifier              0.7890    0.663         0.337   0.66000  \n",
       "relation_extraction     0.5620    0.595         0.411   0.60000  \n",
       "semantic_role           0.6580    0.594         0.594   0.59000  \n",
       "sentiment               0.7840    0.500         0.500   0.50000  \n",
       "rte                     0.5523    0.473         0.516   0.52710  \n",
       "mnli                    0.3487    0.634         0.366   0.34790  \n",
       "ner_person              0.5620    0.547         0.548   0.55000  \n",
       "ner_entity              0.4920    0.422         0.577   0.58000  \n",
       "ner_location            0.4400    0.644         0.358   0.64000  \n",
       "ner_event               0.7710    0.500         0.500   0.50000  \n",
       "ner_organization        0.5910    0.498         0.502   0.50000  \n",
       "ner_time                0.7160    0.655         0.654   0.65000  \n",
       "ner_merged              0.5420    0.488         0.511   0.51000  \n",
       "sst2                    0.6823      NaN           NaN   0.50917  \n",
       "cola                    0.4463      NaN           NaN   0.69090  \n",
       "mrpc                    0.6139      NaN           NaN   0.66400  \n",
       "qnli                    0.5348      NaN           NaN   0.50530  \n",
       "qqp                     0.5283      NaN           NaN   0.63180  \n",
       "cb                      0.6607      NaN           NaN   0.50000  \n",
       "boolq                   0.5403      NaN           NaN   0.62170  \n",
       "copa                    0.4300      NaN           NaN   0.55000  \n",
       "wic                     0.5110      NaN           NaN   0.50000  \n",
       "ner_rand_0              0.4990      NaN           NaN       NaN  \n",
       "ner_rand_1              0.6010      NaN           NaN       NaN  \n",
       "ner_rand_2              0.5570      NaN           NaN       NaN  \n",
       "ner_rand_3              0.6000      NaN           NaN       NaN  \n",
       "ner_rand_4              0.4850      NaN           NaN       NaN  \n",
       "ner_rand_5              0.4850      NaN           NaN       NaN  \n",
       "ner_bert_0              0.5130      NaN           NaN       NaN  \n",
       "ner_bert_1              0.5320      NaN           NaN       NaN  \n",
       "ner_bert_2              0.5190      NaN           NaN       NaN  \n",
       "ner_bert_3              0.5030      NaN           NaN       NaN  \n",
       "ner_bert_4              0.5020      NaN           NaN       NaN  \n",
       "ner_bert_5              0.5080      NaN           NaN       NaN  \n",
       "wnli                       NaN      NaN           NaN   0.56330  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop these as they are not 'accuracy of one model after finetuning'\n",
    "# which is what we need\n",
    "fine_tuned_acc = pd.DataFrame(output_collection) \\\n",
    "    .drop([\"sensitivity\", \"rda\", \"few-avg\"], axis=\"columns\") / 100.0\n",
    "\n",
    "baseline_acc = pd.DataFrame(\n",
    "    { \"baseline\": list(majority_collection.values()) }\n",
    "    , index=list(majority_collection.keys()) )\n",
    "\n",
    "# make sure all values are on a scale of [0.0, 1.0]\n",
    "baseline_acc_scaled = baseline_acc.where(baseline_acc <= 1.0, baseline_acc / 100)\n",
    "all_acc = pd.concat([fine_tuned_acc, baseline_acc_scaled], axis=\"columns\")\n",
    "\n",
    "# I'm not sure what to do with tasks that contain NaN values. They seem in\n",
    "# appropriate to include since we cannot compare all of them with one another.\n",
    "clean_acc = all_acc.dropna()\n",
    "\n",
    "display(all_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Average 'Method-Specific' Few-Shot Hardness on all tasks:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adapet</th>\n",
       "      <td>1.342163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adapter</th>\n",
       "      <td>1.280592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitfit</th>\n",
       "      <td>1.129623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compacter</th>\n",
       "      <td>1.032645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lmbff</th>\n",
       "      <td>1.364306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manual</th>\n",
       "      <td>1.224082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nullp</th>\n",
       "      <td>1.149002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptuning</th>\n",
       "      <td>1.184893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.007012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta-zero</th>\n",
       "      <td>0.892281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              baseline_accuracy\n",
       "method                         \n",
       "adapet                 1.342163\n",
       "adapter                1.280592\n",
       "bitfit                 1.129623\n",
       "compacter              1.032645\n",
       "lmbff                  1.364306\n",
       "manual                 1.224082\n",
       "nullp                  1.149002\n",
       "ptuning                1.184893\n",
       "roberta                1.007012\n",
       "roberta-zero           0.892281"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Average 'Method-Specific' Few-Shot Hardness on 'clean' tasks:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adapet</th>\n",
       "      <td>1.363491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adapter</th>\n",
       "      <td>1.307752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitfit</th>\n",
       "      <td>1.123960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compacter</th>\n",
       "      <td>1.050303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lmbff</th>\n",
       "      <td>1.394720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manual</th>\n",
       "      <td>1.224082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nullp</th>\n",
       "      <td>1.132974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptuning</th>\n",
       "      <td>1.240978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.007012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta-zero</th>\n",
       "      <td>0.892281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              baseline_accuracy\n",
       "method                         \n",
       "adapet                 1.363491\n",
       "adapter                1.307752\n",
       "bitfit                 1.123960\n",
       "compacter              1.050303\n",
       "lmbff                  1.394720\n",
       "manual                 1.224082\n",
       "nullp                  1.132974\n",
       "ptuning                1.240978\n",
       "roberta                1.007012\n",
       "roberta-zero           0.892281"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ptuning</th>\n",
       "      <td>-0.056085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lmbff</th>\n",
       "      <td>-0.030413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adapter</th>\n",
       "      <td>-0.027160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adapet</th>\n",
       "      <td>-0.021329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compacter</th>\n",
       "      <td>-0.017658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manual</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta-zero</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitfit</th>\n",
       "      <td>0.005663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nullp</th>\n",
       "      <td>0.016029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              baseline_accuracy\n",
       "method                         \n",
       "ptuning               -0.056085\n",
       "lmbff                 -0.030413\n",
       "adapter               -0.027160\n",
       "adapet                -0.021329\n",
       "compacter             -0.017658\n",
       "manual                 0.000000\n",
       "roberta                0.000000\n",
       "roberta-zero           0.000000\n",
       "bitfit                 0.005663\n",
       "nullp                  0.016029"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mfh(data) -> pd.DataFrame:\n",
    "    return data.div(data[\"baseline\"], axis=\"index\").drop(\"baseline\", axis=\"columns\")\n",
    "\n",
    "def mfh_avg(data) -> pd.DataFrame:\n",
    "    mfh_data = mfh(data)\n",
    "    molten = mfh_data.melt(\n",
    "        id_vars=None, \n",
    "        value_vars=mfh_data.columns.to_list(), \n",
    "        var_name=\"method\", \n",
    "        value_name=\"baseline_accuracy\")\n",
    "    return molten.groupby(\"method\").mean()\n",
    "\n",
    "all_mfh = mfh_avg(all_acc)\n",
    "clean_mfh = mfh_avg(clean_acc)\n",
    "\n",
    "display(\"Average 'Method-Specific' Few-Shot Hardness on all tasks:\")\n",
    "display(all_mfh)\n",
    "display(\"Average 'Method-Specific' Few-Shot Hardness on 'clean' tasks:\")\n",
    "display(clean_mfh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "def isna_proportion(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"returns the portion of NaN values in each column of the dataframe\" \n",
    "    return pd.DataFrame({ \"isna\" : data.isnull().mean() }, index=data.columns.to_list())\n",
    "\n",
    "diff_mfh = (all_mfh - clean_mfh).sort_values(\"baseline_accuracy\")\n",
    "diff_isna = pd.concat([diff_mfh, isna_proportion(all_acc)], axis=\"columns\")\n",
    "\n",
    "_ = sns.scatterplot(diff_isna, x=\"baseline_accuracy\", y=\"isna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_generation(aspects=[\"cola\",\"sst2\",\"mrpc\",\"qqp\",\"mnli\",\"qnli\",\"rte\",\"boolq\",\"cb\",\"copa\",\"wic\"] \\\n",
    "                       , scope=\"clusters\", mode = \"corr\"):\n",
    "\n",
    "    method_cluster = {}\n",
    "    method_cluster[\"prompt-fs\"] = [\"lmbff\",\"nullp\",\"bitfit\",\"adapet\"]\n",
    "    method_cluster[\"lw-fs\"] = [\"ptuning\",\"compacter\"]\n",
    "    method_cluster[\"fewshot\"] = [\"lmbff\",\"nullp\",\"bitfit\",\"adapet\",\"ptuning\",\"compacter\"]\n",
    "    # method_cluster[\"fewshot-avg\"] = [\"few-avg\"] \n",
    "#     method_cluster[\"lw-ft\"] = [\"p-tuning(f)\",\"adapter(f)\"]\n",
    "#     method_cluster[\"lm-ft\"] = [\"roberta-b(f)\",\"roberta-l(f)\",\"bert-b(f)\",\"bert-l(f)\",\"t5-b(f)\",\"t5-l(f)\",\"t5-11b(f)\"]\n",
    "#     method_cluster[\"human\"] = [\"human\"]\n",
    "\n",
    "\n",
    "\n",
    "    included_tasks = [\"cola\",\"sst2\",\"mrpc\",\"qqp\",\"mnli\",\"qnli\",\"rte\",\"boolq\",\"cb\",\"copa\",\"wic\"]\n",
    "\n",
    "\n",
    "    included_methods = []\n",
    "    for k,v in method_cluster.items():\n",
    "        for item in list(v):\n",
    "            if item not in included_methods:      \n",
    "                included_methods += [item]\n",
    "    \n",
    "    # included_tasks = aspects\n",
    "    \n",
    "    head = [\"methods\"]\n",
    "    \n",
    "    def generate_score(mode,lst1,lst2):\n",
    "        if mode == \"corr\":\n",
    "            return spearmanr(lst1, lst2)[0]\n",
    "            # return pearsonr(lst1, lst2)[0]\n",
    "        elif mode == \"kendall_tau\":\n",
    "            return kendalltau(np.argsort(lst1), np.argsort(lst2))[0]\n",
    "        elif mode == \"weighted_tau\":\n",
    "            return weightedtau(np.argsort(lst1), np.argsort(lst2))[0]\n",
    "        \n",
    "    def cluster_score_avg(cluster1, cluster2, task_collector):\n",
    "        all_score = 0\n",
    "        for v1 in cluster1:\n",
    "            for v2 in cluster2:\n",
    "                all_score += task_collector[v1][v2]\n",
    "                \n",
    "        return all_score/(len(cluster1)*len(cluster2))\n",
    "    \n",
    "    def extract_score(method,task):\n",
    "        temp_score = 0\n",
    "        temp_collection = {}\n",
    "        if method in output_collection.keys():\n",
    "            temp_collection = output_collection\n",
    "        else:\n",
    "            temp_collection = output_dic\n",
    "            \n",
    "        if method in [\"sensitivity\",\"diversity\"]:\n",
    "            temp_score -= temp_collection[method][task]*100\n",
    "        elif method in [\"rda\",\"coverage\"]:\n",
    "            # rda is computed as a sum over the examples\n",
    "            temp_score -= temp_collection[method][task]\n",
    "        else:\n",
    "            temp_score += temp_collection[method][task]-majority_collection[task]\n",
    "                    \n",
    "        return temp_score\n",
    "\n",
    "    # capture the correlation for each two tasks \n",
    "    df = pd.DataFrame()\n",
    "    task_collector = {}\n",
    "    \n",
    "    if scope == \"methods\":\n",
    "        for i,method in enumerate(included_methods):\n",
    "            head.append(method)    \n",
    "        df[\"methods\"] = included_methods\n",
    "    \n",
    "    for method1 in included_methods:\n",
    "        temp = []\n",
    "        task_collector[method1] = {}\n",
    "        \n",
    "        for method2 in included_methods:\n",
    "            lst1 = []\n",
    "            lst2 = []\n",
    "\n",
    "            for task in included_tasks:\n",
    "                lst1.append(extract_score(method1,task))\n",
    "                lst2.append(extract_score(method2,task))\n",
    "\n",
    "            temp_score = generate_score(mode,lst1,lst2)\n",
    "            temp.append(round(temp_score,4))\n",
    "            task_collector[method1][method2] = temp_score \n",
    "            \n",
    "        if scope == \"methods\":\n",
    "            df[method1] = temp\n",
    "            \n",
    "            \n",
    "    if scope == \"methods\":\n",
    "        df.set_index('methods', drop=True, inplace=True)\n",
    "    \n",
    "        \n",
    "    if scope == \"clusters\":\n",
    "            for key in list(method_cluster.keys()):\n",
    "                head.append(key)\n",
    "                \n",
    "            df[\"clusters\"] = list(method_cluster.keys())\n",
    "            \n",
    "            for cluster1 in df[\"clusters\"]:\n",
    "                temp = []\n",
    "                for cluster2 in df[\"clusters\"]:\n",
    "                    temp_score = cluster_score_avg(method_cluster[cluster1], method_cluster[cluster2], task_collector)\n",
    "                    temp.append(round(temp_score,4))\n",
    "            \n",
    "                df[cluster1] = temp\n",
    "\n",
    "            df.set_index('clusters', drop=True, inplace=True)\n",
    "        \n",
    "    mask = np.triu(np.ones_like(df, dtype=bool))\n",
    "    f, ax = plt.subplots(figsize=(9.5, 9))\n",
    "\n",
    "    mask[0][0] = False\n",
    "    mask[1][1] = False\n",
    "    mask[2][2] = False\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    \n",
    "    if scope == \"clusters\":\n",
    "        sns.heatmap(df,annot=True,mask=mask, cmap='gist_gray_r', vmax=1, center=0.78,cbar=False,\n",
    "                    square=False, linewidths=.6, cbar_kws={\"shrink\": .5}, annot_kws={\"fontsize\":50})\n",
    "        \n",
    "        ax.set(ylabel=None)\n",
    "        ax.tick_params(axis='x', labelsize= 30)\n",
    "        ax.tick_params(axis='y', labelsize= 30)\n",
    "    \n",
    "    \n",
    "    if scope == \"methods\":\n",
    "        \n",
    "        mask[3][3] = False\n",
    "        mask[4][4] = False\n",
    "        mask[5][5] = False\n",
    "        sns.heatmap(df,annot=True,mask=mask, cmap='gist_gray_r', vmax=1, center=0.6,cbar=False,\n",
    "            square=False, linewidths=.6, cbar_kws={\"shrink\": .5}, annot_kws={\"fontsize\":30})\n",
    "        \n",
    "        ax.set(ylabel=None)\n",
    "        ax.tick_params(axis='x', labelsize= 15)\n",
    "        ax.tick_params(axis='y', labelsize= 15)\n",
    "\n",
    "    \n",
    "    # image_format = 'svg' # e.g .png, .svg, etc.\n",
    "    # image_name = 'glue_intrinsic_'+scope[:-1]+'.svg'\n",
    "    # f.savefig(image_name, format=image_format, dpi=1200)\n",
    "    \n",
    "    image_name = 'glue_intrinsic_'+scope[:-1]+'.pdf'\n",
    "    f.savefig(image_name, format=\"pdf\", bbox_inches=\"tight\", dpi=1200)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "heatmap_generation(scope=\"methods\")\n",
    "heatmap_generation(scope=\"clusters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
