{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messing Around - Experiments and other Shenanigans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing the _Spread_ metric introduced by Zhao at al. (2022)\n",
    "\n",
    "I know they're using the model _SimCSE with RoBERTa-large base_ which is just\n",
    "available on GitHub and Huggingface Transformers. The most annoying thing right\n",
    "now is to determine what is meant with the following sentence:\n",
    "\n",
    "> For some input $z = (x, y) \\in D$, let $f(z)$ denote the vector valued\n",
    "> features of the input x produced by the model.\n",
    "\n",
    "What are the _vector-valued features_? Currently, I think these represent the\n",
    "`embeddings` as used in the following code snippet given in the [example\n",
    "tutorial](https://github.com/princeton-nlp/SimCSE) of the SimCSE BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A child is skateboarding.\" is: 0.933\n",
      "Cosine similarity between \"There's a kid on a skateboard.\" and \"A child resides inside the house.\" is: 0.367\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Import our models. The package will take care of downloading the models automatically\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"There's a kid on a skateboard.\",\n",
    "    \"A child is skateboarding.\",\n",
    "    \"A child resides inside the house.\"\n",
    "]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pooler_output` seems to refer to the value of the classification token, `[CLS]`, in\n",
    "the BERT-type model. I wasn't sure why this output is particularly important\n",
    "because it resides as the beginning of the sentence, but given that bert is an\n",
    "_encoder only_ network this makes sense now: while typical _decoder only_\n",
    "transformers only look back, encoders look back as well as ahead. The final\n",
    "result of the classification task of the BERT model thus resides in the first\n",
    "token before the classified sentence, the `[CLS]` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is the case that these values are used instead of the tokenizer embeddings\n",
    "as I first thought, that means the method is more specific to this particular\n",
    "model architecture than I would like. Still, in the [`transformers`\n",
    "documentation](https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling.pooler_output)\n",
    "it seems like this is not only a feature of BERT models, so it should probably\n",
    "be fine. I do need to try this out with a different model type tho. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fiddling for a bit, I've concluded the following. While the\n",
    "`pooler_output` is not unique to BERT models, it does not seem like the value\n",
    "necessary for the distance in the paper. It is a single vector while they are\n",
    "talking about multiple features, each of which is a vector. Therefore, we expect\n",
    "to take some matrix norm (since the output of $f(z)$ would then be a matrix).\n",
    "Here is my attempt at reconstructing the method they seem to have used in the\n",
    "paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between \"There's a kid on a skateboard.\" and \"A child is skateboarding.\" is: 4.552\n",
      "Distance between \"There's a kid on a skateboard.\" and \"A child resides inside the house.\" is: 3.318\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Import our models. The package will take care of downloading the models\n",
    "# automatically. NOTE: don't use seq2seq.. they're really annoying.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "model = AutoModel.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"There's a kid on a skateboard.\",\n",
    "    \"A child is skateboarding.\",\n",
    "    \"A child resides inside the house.\"\n",
    "]\n",
    "tokenizer.pad_token = \" \" # NOTE: I don't know whether this is legit\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings: torch.Tensor = model(**inputs, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "\n",
    "# Calculate distance as defined in the paper\n",
    "distance_0_1 = (embeddings[0] - embeddings[1]).norm()\n",
    "distance_0_2 = (embeddings[0] - embeddings[2]).norm()\n",
    "\n",
    "print(\"Distance between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], distance_0_1))\n",
    "print(\"Distance between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], distance_0_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is not an equivalent to the method shown before (no cosine\n",
    "similarity), this seems like a reasonable substitute for it and I don't find it\n",
    "unreasonable to imageine that the authors would have gone through similar steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Token Embeddings\n",
    "\n",
    "One thing I am curious about to see how much the upper method compares to simply\n",
    "using the embeddings generated by the tokenizer. Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between \"There's a kid on a skateboard.\" and \"A child is skateboarding.\" is: 80844.202\n",
      "Distance between \"There's a kid on a skateboard.\" and \"A child resides inside the house.\" is: 77876.963\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Import our models. The package will take care of downloading the models\n",
    "# automatically. NOTE: don't use seq2seq.. they're really annoying.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "model = AutoModel.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"There's a kid on a skateboard.\",\n",
    "    \"A child is skateboarding.\",\n",
    "    \"A child resides inside the house.\"\n",
    "]\n",
    "tokenizer.pad_token = \" \" # NOTE: I don't know whether this is legit\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Calculate distance as defined in the paper\n",
    "distance_0_1 = (inputs[0] - inputs[1]).double().norm()\n",
    "distance_0_2 = (inputs[0] - inputs[2]).double().norm()\n",
    "\n",
    "print(\"Distance between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], distance_0_1))\n",
    "print(\"Distance between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], distance_0_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah yes, no, absolutely not..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Padding Matters\n",
    "\n",
    "One thing I do notice is that, when considering the distance, the padding used\n",
    "to make a single batch matters quite a bit. I guess this wasn't an issue before,\n",
    "but the `tiny-gpt` model doesn't have a built-in padding token. I can iterate\n",
    "over all the different ascii characters and see what their impact is on the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spread in difference between two sentences: \n",
      "  1. \"There's a kid on a skateboard.\"\n",
      "  2. \"A child is skateboarding.\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_diff_with_padding(padding=\" \"):\n",
    "    tokenizer.pad_token = padding\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings: torch.Tensor = model(**inputs, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "\n",
    "    # Calculate distance as defined in the paper\n",
    "    distance_0_1 = (embeddings[0] - embeddings[1]).norm().item()\n",
    "    distance_0_2 = (embeddings[0] - embeddings[2]).norm().item()\n",
    "\n",
    "    return abs(distance_0_1 - distance_0_2)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "results = pd.DataFrame({ \"char\" : list(string.printable) })\n",
    "results.insert(0, column=\"diff\", value=results[\"char\"].map(get_diff_with_padding))\n",
    "results = results.sort_values(by=\"diff\")\n",
    "\n",
    "_ = sns.boxplot(results, y=\"diff\")\n",
    "\n",
    "print(f\"Spread in difference between two sentences: \\n  1. \\\"{texts[0]}\\\"\\n  2. \\\"{texts[1]}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the average distance of these two sentences are around 1.2, a spread\n",
    "of .5 does not seem insignificant. Does this matter? Maybe not, but it is\n",
    "strange. If I would like this to be compatible with various models I should\n",
    "first figure out how this is generally dealt with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Now that we effectively have the _Spread_ measure (calculating it fully should\n",
    "not be a problem as we now have all the puzzle pieces), what is the next step?\n",
    "We could do a couple of things:\n",
    "\n",
    "- Compare this measure with that of the **HardenedMetaSet++** technique.\n",
    "- Show whether this has any value in terms of security\n",
    "- Continue working on reproducing the paper\n",
    "\n",
    "While I'm not sure which would be the most effective to work on, I think the\n",
    "second is the most important to the thesis itself. I'm aware of some stuff in\n",
    "the current paper that might be reason for concern, but I will let those be for\n",
    "now. Let's, for now, assume that the current measure is the end-all and be-all\n",
    "of few-shot dataset hardness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with GPTFuzzer by Yu et al. (2023)\n",
    "\n",
    "To that end, I would like to be able to run my own test using the method used in\n",
    "_GPTFuzzer_. The reason for this tool specifically, is because it generated new\n",
    "samples from old ones, specifically attacking the adaptability we are trying to\n",
    "establish. The tool seems really easy to work with and is fully available on\n",
    "github. It's slightly annoying that it's not available as a package, but this is\n",
    "my current workaround, maybe I'll submit a PR that makes it easier to interact\n",
    "with other code.\n",
    "\n",
    "I switched over to using conda environments because the paper uses this, and I\n",
    "think it's a good idea given the amount of packages I'll have to deal with.\n",
    "Furthermore, I'm working in a `devcontainer` which means that not all kernel\n",
    "features are (readily) available, so too for my NVidia GPU. I've followed [this\n",
    "stackoverflow\n",
    "thread](https://stackoverflow.com/questions/72129213/using-gpu-in-vs-code-container) \n",
    "to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"gptfuzz\"\n",
    "assert sys.version_info[:2] == (3, 8)\n",
    "\n",
    "%conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "%pip install fschat vllm openai termcolor openpyxl google-generativeai anthropic accelerate chardet pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it does work, but sadly you need credits on your OpenAI account to be able to\n",
    "run the thing... Not sure what the best next step is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:37:19] Fuzzing started!\n",
      "/opt/conda/envs/gptfuzz/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "[10:37:38] Fuzzing interrupted by user!\n",
      "[10:37:38] Fuzzing finished!\n"
     ]
    }
   ],
   "source": [
    "# This solves gptfuzz needing to be executed in the folder containing the\n",
    "# `gptfuzzer` folder. Python module resolution is something else...\n",
    "import sys\n",
    "sys.path.insert(1, './gptfuzz/')\n",
    "\n",
    "import gptfuzz\n",
    "import torch\n",
    "\n",
    "# first, empty cache... I don't have much memory :(\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory._record_memory_history()\n",
    "\n",
    "# '--openai_key', type=str, default='', help='OpenAI API Key'\n",
    "# '--claude_key', type=str, default='', help='Claude API Key'\n",
    "# '--palm_key', type=str, default='', help='PaLM2 api key'\n",
    "# '--model_path', type=str, default='gpt-3.5-turbo', help='mutate model path'\n",
    "# '--target_model', type=str, default='meta-llama/Llama-2-7b-chat-hf', help='The target model, openai model or open-sourced LLMs'\n",
    "# '--max_query', type=int, default=1000, help='The maximum number of queries'\n",
    "# '--max_jailbreak', type=int, default=1, help='The maximum jailbreak number'\n",
    "# '--energy', type=int, default=1, help='The energy of the fuzzing process'\n",
    "# '--seed_selection_strategy', type=str, default='round_robin', help='The seed selection strategy'\n",
    "# '--max-new-tokens', type=int, default=512)\n",
    "# '--seed_path', type=str, default=\"datasets/prompts/GPTFuzzer.csv\"\n",
    "\n",
    "env = {}\n",
    "\n",
    "with open(\".env\", \"r\") as env_file:\n",
    "    for line in env_file.readlines():\n",
    "        key, value = line.split(\"=\")\n",
    "        env[key] = value\n",
    "\n",
    "class FuzzerArgs():\n",
    "    # the default model is gated, meaning you need permission to access it\n",
    "    # target_model =  \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    # target_model =  \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    target_model = \"ahxt/LiteLlama-460M-1T\"\n",
    "    target_model_args = {\n",
    "        \"dtype\" : torch.float16\n",
    "    }\n",
    "    model_path = \"gpt-3.5-turbo\"\n",
    "    seed_path = \"gptfuzz/datasets/prompts/GPTFuzzer.csv\"\n",
    "    openai_key = env[\"OPENAIKEY\"]\n",
    "    energy = 1\n",
    "    max_jailbreak = 1\n",
    "    max_query = 1000\n",
    "\n",
    "try: \n",
    "    gptfuzz.main(FuzzerArgs())\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"Process crashed due to lack of memory resources...\")\n",
    "    torch.cuda.memory._dump_snapshot(\"gpu_usage.pickle\")\n",
    "finally:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Red-Teaming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
